{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# PenaltyPlay Project Overview"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "This notebook provides a quick walkthrough of the repository. It covers setting up the environment, training the agents, evaluating the models, visualizing results, and running tests."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1. Install Dependencies"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Install the required Python packages using `pip` and the `requirements.txt` file."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "!pip install -r requirements.txt"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2. Train the Agents"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Run the training script to create striker and goalkeeper models. Training uses parameters from `configs/agent_params.yaml`."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "!python train.py"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3. Evaluate the Models"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "After training, evaluate the saved models with the evaluation script. Replace the paths with the model checkpoints you wish to evaluate."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "!python evaluate.py --striker models/striker_episode_100 --goalkeeper models/goalkeeper_episode_100"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4. Visualize Results"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Use the visualization utilities to plot training metrics or evaluation outcomes. Below is an example of plotting evaluation metrics returned by `evaluate.py`."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from utils.visualize import plot_evaluation_results\nresults = {'mean_reward': 1.0, 'success_rate': 0.6, 'save_rate': 0.4}\nplot_evaluation_results(results)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5. Run Tests"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Execute the unit tests to ensure the environment behaves as expected."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "!pytest -q"
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 2
}
